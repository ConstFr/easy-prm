type: 'deberta'
model_id: 'microsoft/deberta-v3-large'
train_data_path: './data/mmlu_auged.json'
eval_data_path: './data/subsets/mmlu_small_auged_eval_16.json'
max_length: 750
use_augs: false
train_label_last_n: 1
eval_label_last_n: 1
wandb_project: 'prm_train'

training_args:
  output_dir: './runs/deberta_prm_1'
  report_to: "wandb"

  learning_rate: 1.0e-5
  gradient_accumulation_steps: 2
  lr_scheduler_type: 'cosine' 
  warmup_ratio: 0.1
  bf16: true
  per_device_train_batch_size: 4
  num_train_epochs: 1
  weight_decay: 0.01
  # gradient_checkpointing: true

  eval_accumulation_steps: 3
  per_device_eval_batch_size: 4
  eval_strategy: 'steps'
  eval_steps: 50
  eval_strategy: 'steps'

  logging_steps: 1
  save_strategy: 'steps'
  save_steps: 500
  save_total_limit: 1
  load_best_model_at_end: false
  push_to_hub: false
  hub_strategy: 'checkpoint'
  seed: 908932403
  data_seed: 289245
  use_liger_kernel: false


# OPT: AdamW, Muon, SGD
# BS: [2, 4 , 16]
# LR: [5e-6, 1e-6, 1e-5, 5e-5, 1e-4, 5e-4]
# AdamW: B_1, B_2 (not sensitive)
# Muon: \gamma, momentum

# 4 * 24 / 2 * 2 = 112